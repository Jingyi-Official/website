## GPT Series and Large Language Models
- Tag: Large Language Models
- Summary: This blog explores the evolution of artificial intelligence from Narrow AI to Artificial General Intelligence (AGI), with a focus on Large Language Models (LLMs) like GPT. A deep dive into the transformer architecture is provided, highlighting key concepts such as attention mechanisms, self-attention, multi-head attention, feed-forward networks, positional encoding, and layer normalization.  It outlines how models have advanced from task-specific designs to transformer-based architectures capable of handling diverse data types. The progression of GPT models is then traced from GPT-1 to GPT-4.
- Link: https://jingyiwan.com/large-language-models

## Generative Models
- Tag: Generative Modelling
- Summary: This blog explores the fundamentals and taxonomy of generative models, including autoregressive models, variational autoencoders (VAEs), and generative adversarial networks (GANs). It discusses how these models learn data distributions, their underlying mechanisms, strengths and limitations, as well as evaluation techniques such as likelihood estimation and Frechet Inception Distance.
- Link: https://jingyiwan.com/generative-models
